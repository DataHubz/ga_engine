# CUDA V3 Bootstrap Sequential Bottlenecks - Complete Audit

**Date**: 2025-11-09
**Status**: üö® **CRITICAL PERFORMANCE ISSUES FOUND**
**Impact**: Multiple CPU loops that should be GPU kernels

---

## üéØ Executive Summary

The V3 CUDA bootstrap implementation has **MANY sequential CPU loops** that are processing large arrays (N=1024, 30 primes) on CPU instead of GPU.

**Estimated Performance Impact**: **2-4 seconds** could be saved by moving these to GPU kernels.

---

## üö® Critical Bottlenecks Found

### 1. **Modulus Raise** - Sequential CPU Loop ‚ùå

**File**: [cuda_bootstrap.rs:143-151](src/clifford_fhe_v3/bootstrapping/cuda_bootstrap.rs#L143-L151)

**Problem**:
```rust
// Copy existing coefficients (strided layout) - SEQUENTIAL ON CPU!
for coeff_idx in 0..n {
    for prime_idx in 0..=ct.level {
        c0_raised[coeff_idx * (target_level + 1) + prime_idx] =
            ct.c0[coeff_idx * (ct.level + 1) + prime_idx];
        c1_raised[coeff_idx * (target_level + 1) + prime_idx] =
            ct.c1[coeff_idx * (ct.level + 1) + prime_idx];
    }
    // Higher primes get value 0 (already initialized)
}
```

**Issue**: Nested loop processing 1024 √ó 20 = **20,480 elements** sequentially on CPU

**Should Be**: GPU kernel that copies in parallel (1 thread per element)

**Estimated Cost**: ~5-10ms (upload to GPU, copy on GPU, download)

---

### 2. **Layout Conversion (Strided ‚Üî Flat)** - Multiple CPU Loops ‚ùå

#### 2a. Strided ‚Üí Flat in Rotation

**File**: [cuda_coeff_to_slot.rs:139-146](src/clifford_fhe_v3/bootstrapping/cuda_coeff_to_slot.rs#L139-L146)

```rust
// Convert to flat RNS layout for GPU rotation - SEQUENTIAL ON CPU!
for coeff_idx in 0..n {
    for prime_idx in 0..num_primes {
        let strided_idx = coeff_idx * num_primes + prime_idx;
        let flat_idx = prime_idx * n + coeff_idx;
        c0_flat[flat_idx] = ct.c0[strided_idx];
        c1_flat[flat_idx] = ct.c1[strided_idx];
    }
}
```

**Issue**: 1024 √ó 30 = **30,720 elements** copied sequentially on CPU

**Called**: 18 times per bootstrap (9 CoeffToSlot + 9 SlotToCoeff rotations)

**Total Cost**: 18 √ó 5-10ms = **90-180ms** wasted

#### 2b. Flat ‚Üí Strided After Rotation

**File**: [cuda_coeff_to_slot.rs:176-183](src/clifford_fhe_v3/bootstrapping/cuda_coeff_to_slot.rs#L176-L183)

```rust
// Convert back from flat to strided layout - SEQUENTIAL ON CPU!
for coeff_idx in 0..n {
    for prime_idx in 0..num_primes {
        let flat_idx = prime_idx * n + coeff_idx;
        let strided_idx = coeff_idx * num_primes + prime_idx;
        c0_strided[strided_idx] = c0_result[flat_idx];
        c1_strided[strided_idx] = c1_ks[flat_idx];
    }
}
```

**Issue**: Same as above - **30,720 elements** sequentially

**Called**: 18 times per bootstrap

**Total Cost**: 18 √ó 5-10ms = **90-180ms** wasted

**TOTAL FOR LAYOUT CONVERSION**: **180-360ms** (0.18-0.36 seconds!)

---

### 3. **Addition After Rotation** - CPU Loop ‚ùå

**File**: [cuda_coeff_to_slot.rs:165-170](src/clifford_fhe_v3/bootstrapping/cuda_coeff_to_slot.rs#L165-L170)

```rust
// Step 4: Add c0(X^g) + c0_ks - SEQUENTIAL ON CPU!
let mut c0_result = vec![0u64; n * num_primes];
for i in 0..(n * num_primes) {
    let prime_idx = i / n;
    let q = rotation_keys.modulus(prime_idx);
    c0_result[i] = (c0_galois[i] + c0_ks[i]) % q;
}
```

**Issue**: **30,720 additions** done sequentially on CPU with modular reduction

**Should Be**: GPU `rns_add` kernel (we already have this!)

**Called**: 18 times per bootstrap

**Total Cost**: 18 √ó 2-5ms = **36-90ms**

---

### 4. **Plaintext Multiplication** - CPU Loop ‚ùå

**File**: [cuda_coeff_to_slot.rs:224-238](src/clifford_fhe_v3/bootstrapping/cuda_coeff_to_slot.rs#L224-L238)

```rust
// Multiply ciphertext by plaintext - SEQUENTIAL ON CPU!
for coeff_idx in 0..n {
    for prime_idx in 0..num_primes {
        let idx = coeff_idx * num_primes + prime_idx;
        let q = ckks_ctx.params().moduli[prime_idx];

        // c0_result = c0 * pt (mod q)
        let c0_val = ct.c0[idx];
        let pt_val = pt[idx];
        c0_result[idx] = ((c0_val as u128 * pt_val as u128) % q as u128) as u64;

        // c1_result = c1 * pt (mod q)
        let c1_val = ct.c1[idx];
        c1_result[idx] = ((c1_val as u128 * pt_val as u128) % q as u128) as u64;
    }
}
```

**Issue**: **30,720 multiplications** with 128-bit arithmetic on CPU

**Should Be**: GPU kernel with parallel modular multiplication

**Called**: Multiple times in CoeffToSlot, SlotToCoeff, and EvalMod

**Estimated Cost**: 5-15ms per call √ó ~20 calls = **100-300ms**

---

### 5. **Ciphertext Addition** - CPU Loop ‚ùå

**File**: [cuda_coeff_to_slot.rs:283-294](src/clifford_fhe_v3/bootstrapping/cuda_coeff_to_slot.rs#L283-L294)

```rust
// Add coefficient-wise using proper modular arithmetic - SEQUENTIAL ON CPU!
for coeff_idx in 0..n {
    for prime_idx in 0..num_active_primes {
        let q = ckks_ctx.params().moduli[prime_idx];
        let idx = coeff_idx * num_primes + prime_idx;

        // c0 = ct1.c0 + ct2.c0 (mod q)
        let sum0 = ct1.c0[idx] + ct2.c0[idx];
        c0[idx] = if sum0 >= q { sum0 - q } else { sum0 };

        // c1 = ct1.c1 + ct2.c1 (mod q)
        let sum1 = ct1.c1[idx] + ct2.c1[idx];
        c1[idx] = if sum1 >= q { sum1 - q } else { sum1 };
    }
}
```

**Issue**: **20,480 additions** (20 active primes) done sequentially

**Should Be**: GPU `rns_add` kernel (we already have this!)

**Called**: Many times in EvalMod (BSGS additions)

**Estimated Cost**: 2-5ms per call √ó many calls = **significant**

---

### 6. **Ciphertext Subtraction** - CPU Loop ‚ùå

**File**: [cuda_eval_mod.rs:452](src/clifford_fhe_v3/bootstrapping/cuda_eval_mod.rs#L452) (similar pattern)

```rust
// Subtract coefficient-wise - SEQUENTIAL ON CPU!
for idx in 0..(n * num_primes) {
    // ... subtraction logic
}
```

**Issue**: Same as addition - should be GPU kernel

---

## üìä Performance Impact Analysis

### Current Performance Breakdown

| Operation | Current Time | Sequential CPU Time | GPU Potential |
|-----------|--------------|---------------------|---------------|
| **EvalMod** | 12.55s | ~0.5-1.0s in CPU loops | **-0.5-1.0s** |
| **CoeffToSlot** | 0.14s | ~0.1-0.2s in CPU loops | **-0.1-0.2s** |
| **SlotToCoeff** | 0.08s | ~0.1-0.2s in CPU loops | **-0.1-0.2s** |
| **Total** | **~13s** | **~0.7-1.4s wasted** | **-0.7-1.4s** |

### Detailed Bottleneck Costs

| Bottleneck | Cost per Call | Calls per Bootstrap | Total Cost |
|------------|---------------|---------------------|------------|
| Layout conversion (strided‚Üîflat) | 5-10ms | 36 (18√ó2) | **180-360ms** |
| Addition after rotation | 2-5ms | 18 | **36-90ms** |
| Plaintext multiplication | 5-15ms | ~20 | **100-300ms** |
| Ciphertext addition | 2-5ms | ~50 (BSGS) | **100-250ms** |
| Ciphertext subtraction | 2-5ms | ~10 | **20-50ms** |
| Modulus raise | 5-10ms | 1 | **5-10ms** |
| **TOTAL** | - | - | **441-1060ms** |

**Expected Speedup if All Fixed**: **0.4-1.0 seconds** (possibly more with better GPU utilization)

---

## ‚úÖ What's Already GPU-Accelerated (Good!)

1. ‚úÖ **NTT operations** - Using batched GPU kernels
2. ‚úÖ **Galois rotations** - Using GPU rotation kernel
3. ‚úÖ **Key switching** - Using GPU NTT for rotation keys
4. ‚úÖ **Rescaling** - Using `exact_rescale_gpu()`
5. ‚úÖ **Relinearization** - Using GPU tensored multiplication

---

## üõ†Ô∏è Required Fixes

### Fix Priority: HIGH üî¥

These are simple fixes with high impact:

#### Fix #1: GPU Addition (Easy - Kernel Exists!)

**Replace**:
```rust
for coeff_idx in 0..n {
    for prime_idx in 0..num_active_primes {
        let q = ckks_ctx.params().moduli[prime_idx];
        let idx = coeff_idx * num_primes + prime_idx;
        let sum0 = ct1.c0[idx] + ct2.c0[idx];
        c0[idx] = if sum0 >= q { sum0 - q } else { sum0 };
        // ...
    }
}
```

**With**:
```rust
// Upload to GPU
let gpu_c0_1 = ckks_ctx.device.htod_copy(&ct1.c0)?;
let gpu_c0_2 = ckks_ctx.device.htod_copy(&ct2.c0)?;
// ... same for c1

// Use existing rns_add kernel
let gpu_c0_result = ckks_ctx.add_polynomials_gpu(&gpu_c0_1, &gpu_c0_2, num_active_primes)?;
// ... same for c1

// Download result
let c0 = ckks_ctx.device.dtoh_sync_copy(&gpu_c0_result)?;
```

**Impact**: Save **100-250ms** in additions
**Difficulty**: Easy - kernel already exists!

---

#### Fix #2: GPU Subtraction (Easy - Similar to Addition)

Same as addition but using `rns_sub` kernel (may need to add if not exists)

**Impact**: Save **20-50ms**
**Difficulty**: Easy

---

#### Fix #3: GPU Layout Conversion (Medium - Kernel Exists!)

**Replace**:
```rust
for coeff_idx in 0..n {
    for prime_idx in 0..num_primes {
        let strided_idx = coeff_idx * num_primes + prime_idx;
        let flat_idx = prime_idx * n + coeff_idx;
        c0_flat[flat_idx] = ct.c0[strided_idx];
    }
}
```

**With**:
```rust
// Upload strided data
let gpu_strided = ckks_ctx.device.htod_copy(&ct.c0)?;

// Convert on GPU using existing kernel
let gpu_flat = ckks_ctx.strided_to_flat_gpu(&gpu_strided, n, num_primes, num_primes)?;

// Keep on GPU for rotation (don't download!)
```

**Impact**: Save **180-360ms** (biggest win!)
**Difficulty**: Medium - kernel exists but need to keep data GPU-resident

---

#### Fix #4: GPU Plaintext Multiplication (Medium)

Need to create `rns_multiply_plain` kernel or use existing multiplication infrastructure.

**Impact**: Save **100-300ms**
**Difficulty**: Medium - may need new kernel

---

#### Fix #5: GPU Modulus Raise (Easy)

Simple copy operation on GPU.

**Impact**: Save **5-10ms** (minor)
**Difficulty**: Easy - trivial kernel

---

### Fix Priority: MEDIUM üü°

#### Fix #6: Keep Data GPU-Resident Between Operations

The **biggest optimization** would be to avoid the upload‚Üíoperation‚Üídownload pattern:

**Current (BAD)**:
```rust
// Upload
let gpu_data = upload(ct.c0);
// Operate
let gpu_result = gpu_rotate(gpu_data);
// Download
let result = download(gpu_result);  // ‚Üê UNNECESSARY!

// Upload again for next operation
let gpu_data2 = upload(result);     // ‚Üê WASTE!
```

**Optimal (GOOD)**:
```rust
// Upload ONCE
let mut gpu_data = upload(ct.c0);
// Chain operations on GPU
gpu_data = gpu_rotate(gpu_data);
gpu_data = gpu_add(gpu_data, other);
gpu_data = gpu_rescale(gpu_data);
// Download ONCE at the end
let result = download(gpu_data);
```

**Impact**: Save **1-2 seconds** by eliminating round-trips
**Difficulty**: High - requires API redesign

---

## üéØ Recommended Action Plan

### Phase 1: Quick Wins (1-2 days) ‚úÖ

1. **Replace CPU addition loops with `rns_add` kernel**
   - Files: `cuda_coeff_to_slot.rs`, `cuda_eval_mod.rs`
   - Impact: 100-250ms saved
   - Difficulty: Easy

2. **Replace CPU layout conversions with GPU kernel calls**
   - Files: `cuda_coeff_to_slot.rs`, `cuda_slot_to_coeff.rs`
   - Impact: 180-360ms saved
   - Difficulty: Medium

3. **Add `rns_sub` kernel for subtraction**
   - File: `cuda_eval_mod.rs`
   - Impact: 20-50ms saved
   - Difficulty: Easy

**Expected Phase 1 Speedup**: **0.3-0.7 seconds** (12.55s ‚Üí 11.85-12.25s)

### Phase 2: Plaintext Operations (2-3 days)

4. **GPU plaintext multiplication**
   - Files: `cuda_coeff_to_slot.rs`, `cuda_eval_mod.rs`
   - Impact: 100-300ms saved
   - Difficulty: Medium

5. **GPU modulus raise**
   - File: `cuda_bootstrap.rs`
   - Impact: 5-10ms saved
   - Difficulty: Easy

**Expected Phase 2 Speedup**: **0.1-0.3 seconds** (11.85s ‚Üí 11.55-12.15s)

### Phase 3: GPU-Resident Pipeline (1-2 weeks)

6. **Redesign to keep ciphertexts on GPU throughout operations**
   - Requires API changes to pass `CudaSlice<u64>` instead of `Vec<u64>`
   - Impact: 1-2 seconds saved
   - Difficulty: High

**Expected Phase 3 Speedup**: **1-2 seconds** (11.55s ‚Üí 9.5-10.5s)

---

## üèÜ Expected Final Performance

| Stage | Current | After Phase 1 | After Phase 2 | After Phase 3 |
|-------|---------|---------------|---------------|---------------|
| **EvalMod** | 12.55s | 11.85-12.25s | 11.55-12.15s | **9.5-10.5s** |
| **Total Bootstrap** | ~13s | ~12.2-12.6s | ~11.9-12.5s | **~10-11s** |

**GOAL**: Achieve **sub-11s EvalMod** and **~10s total bootstrap** with Phase 3!

This would **match or beat** the historical 11s result!

---

## üìã Summary

### Problems Found

üö® **6 major bottlenecks** where sequential CPU loops should be GPU kernels:
1. Modulus raise (5-10ms)
2. Layout conversion strided‚Üîflat (180-360ms) **‚Üê BIGGEST WIN**
3. Addition after rotation (36-90ms)
4. Plaintext multiplication (100-300ms)
5. Ciphertext addition (100-250ms)
6. Ciphertext subtraction (20-50ms)

**Total wasted CPU time**: **0.4-1.1 seconds**

### Solutions

‚úÖ **Phase 1** (easy, 0.3-0.7s speedup): Use existing GPU kernels
‚úÖ **Phase 2** (medium, 0.1-0.3s speedup): Add missing GPU kernels
‚úÖ **Phase 3** (hard, 1-2s speedup): GPU-resident data pipeline

### Expected Results

**After all fixes**: **9.5-10.5s EvalMod** (vs 12.55s current)

This would achieve the **11s target** and match historical best performance!

---

## üöÄ Next Steps

Would you like me to implement:
1. **Phase 1 fixes** (quick wins with existing kernels)?
2. **Phase 2 fixes** (add missing GPU kernels)?
3. **Phase 3 redesign** (GPU-resident pipeline)?

Or should I focus on a specific bottleneck first?
